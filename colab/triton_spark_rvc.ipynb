{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Triton Spark TTS + RVC Voice Conversion\n",
    "\n",
    "This notebook runs the full pipeline:\n",
    "1. **Triton Spark TTS** - Text-to-Speech (runs inside udocker container)\n",
    "2. **RVC Voice Conversion** - Voice cloning (runs on host Python with CUDA)\n",
    "\n",
    "## Architecture\n",
    "- Triton Server runs Spark TTS with TensorRT-LLM optimization (~10x faster)\n",
    "- RVC runs on host Python with direct CUDA access\n",
    "- Both share GPU memory via cross-process CUDA\n",
    "- Host Python communicates with Triton via gRPC (udocker shares host network)\n",
    "\n",
    "## Requirements\n",
    "- Google Colab with GPU runtime (T4 minimum, A100 recommended)\n",
    "- ~10GB disk space for models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 1: Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1.1: Initialize udocker for Triton container\n",
    "def udocker_init():\n",
    "    import os\n",
    "    if not os.path.exists(\"/home/user\"):\n",
    "        !pip install udocker > /dev/null\n",
    "        !udocker --allow-root install > /dev/null\n",
    "        !useradd -m user > /dev/null\n",
    "    print('Docker-in-Colab initialized')\n",
    "    def execute(command: str):\n",
    "        user_prompt = \"\\033[1;32muser@pc\\033[0m\"\n",
    "        print(f\"{user_prompt}$ udocker {command}\")\n",
    "        !su - user -c \"udocker $command\"\n",
    "    return execute\n",
    "\n",
    "udocker = udocker_init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1.2: Clone repository\n",
    "!git clone https://github.com/VSlobolinskyi/triton-spark-server.git\n",
    "%cd triton-spark-server\n",
    "# Switch to feature branch with RVC integration\n",
    "!git checkout feature/rvc-integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1.3: GPU Configuration\n",
    "# Detect NVIDIA/CUDA paths and create proper volume/env mappings for udocker\n",
    "\n",
    "def detect_gpu_paths():\n",
    "    import os\n",
    "    import subprocess\n",
    "\n",
    "    # Detect critical paths\n",
    "    paths = {}\n",
    "    paths['nvidia_smi'] = subprocess.getoutput('which nvidia-smi')\n",
    "    paths['cuda_dir'] = subprocess.getoutput(\"find /usr -path '*/cuda*' -type d -maxdepth 3 | grep -v 'targets' | head -1\")\n",
    "    paths['nvidia_lib_dir'] = subprocess.getoutput(\"find /usr -name 'libcuda.so*' -o -name 'libnvidia-ml.so*' | grep -v 'stubs' | xargs dirname | sort -u | head -1\")\n",
    "    paths['ld_library_path'] = f\"{paths['nvidia_lib_dir']}:{paths['cuda_dir']}/lib64:{paths['cuda_dir']}/compat\"\n",
    "\n",
    "    # Get basic NVIDIA devices that we know work\n",
    "    basic_devices = ['/dev/nvidia0', '/dev/nvidiactl', '/dev/nvidia-uvm']\n",
    "\n",
    "    # Create volume mappings\n",
    "    volumes = [\n",
    "        f\"--volume={paths['nvidia_smi']}:{paths['nvidia_smi']}\",\n",
    "        f\"--volume={paths['nvidia_lib_dir']}:{paths['nvidia_lib_dir']}\",\n",
    "        f\"--volume={paths['cuda_dir']}:{paths['cuda_dir']}\"\n",
    "    ]\n",
    "\n",
    "    # Add only the basic device mappings that we know work\n",
    "    for device in basic_devices:\n",
    "        if os.path.exists(device):\n",
    "            volumes.append(f\"--volume={device}:{device}\")\n",
    "\n",
    "    # Create environment variables - add TensorRT path within container and include Triton Server path\n",
    "    env_vars = [\n",
    "        f\"--env=LD_LIBRARY_PATH={paths['ld_library_path']}:/usr/local/tensorrt/targets/x86_64-linux-gnu/lib:$LD_LIBRARY_PATH\",\n",
    "        f\"--env=PATH=/opt/tritonserver/bin:{paths['cuda_dir']}/bin:/usr/bin:$PATH\",\n",
    "        \"--env=NVIDIA_VISIBLE_DEVICES=all\",\n",
    "        \"--env=NVIDIA_DRIVER_CAPABILITIES=compute,utility\"\n",
    "    ]\n",
    "\n",
    "    return {\n",
    "        'paths': paths,\n",
    "        'volumes': volumes,\n",
    "        'env_vars': env_vars,\n",
    "        'all_options': ' '.join(volumes + env_vars)\n",
    "    }\n",
    "\n",
    "# Get GPU configuration\n",
    "gpu_config = detect_gpu_paths()\n",
    "\n",
    "# Print the configuration\n",
    "print(\"Detected NVIDIA/CUDA paths:\")\n",
    "for k, v in gpu_config['paths'].items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "print(\"\\nVolume mappings:\")\n",
    "for v in gpu_config['volumes']:\n",
    "    print(f\"  {v}\")\n",
    "\n",
    "print(\"\\nEnvironment variables:\")\n",
    "for e in gpu_config['env_vars']:\n",
    "    print(f\"  {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 2: Setup Triton Container (Spark TTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2.1: Pull Triton Server image\n",
    "!udocker --allow-root pull nvcr.io/nvidia/tritonserver:25.02-trtllm-python-py3\n",
    "!udocker --allow-root create --name=triton_server nvcr.io/nvidia/tritonserver:25.02-trtllm-python-py3\n",
    "!udocker --allow-root setup --nvidia triton_server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2.2: Install dependencies inside container\n",
    "# Note: Repo is mounted at /workspace via --volume=$PWD:/workspace\n",
    "import os\n",
    "pwd = os.getcwd()\n",
    "\n",
    "install_cmd = f'''udocker --allow-root run \\\n",
    "    --hostauth --hostenv \\\n",
    "    {gpu_config['all_options']} \\\n",
    "    --volume={pwd}:/workspace \\\n",
    "    triton_server \\\n",
    "    /bin/bash -c \"apt-get update && apt-get install -y cmake && \\\n",
    "git clone https://github.com/pytorch/audio.git && cd audio && git checkout c670ad8 && USE_FFMPEG=0 PATH=/usr/local/cuda/bin:\\$PATH python3 setup.py develop && \\\n",
    "pip install einx==0.3.0 omegaconf==2.3.0 soundfile==0.12.1 soxr==0.5.0.post1 tritonclient librosa 'huggingface-hub>=0.24.0,<1.0'\"\n",
    "'''\n",
    "\n",
    "!{install_cmd}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2.3: Build TensorRT-LLM engine for Spark TTS\n",
    "# Stage 0: Download model (if not already downloaded in 2.2)\n",
    "# Stage 1: Convert checkpoint to TensorRT weights\n",
    "# Stage 2: Create model repository\n",
    "import os\n",
    "pwd = os.getcwd()\n",
    "\n",
    "setup_cmd = f'''udocker --allow-root run \\\n",
    "    --hostauth --hostenv \\\n",
    "    {gpu_config['all_options']} \\\n",
    "    --env=PYTHONPATH=/workspace \\\n",
    "    --volume={pwd}:/workspace \\\n",
    "    triton_server \\\n",
    "    /bin/bash -c \"cd /workspace/triton && bash run.sh 0 2 offline\"'''\n",
    "\n",
    "!{setup_cmd}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 3: Setup RVC on Host Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3.1: Install RVC dependencies on host\n",
    "\n",
    "# Install fairseq from git (required for HuBERT)\n",
    "!pip install -q git+https://github.com/One-sixth/fairseq.git\n",
    "\n",
    "# Install main requirements\n",
    "!pip install -q -r requirements.txt\n",
    "\n",
    "print(\"RVC dependencies installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3.2: Download RVC assets (HuBERT, RMVPE)\n",
    "!python tools/download_all_assets.py --rvc-only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3.3: Download RVC voice model (SilverWolf example)\n",
    "# You can replace this URL with your own RVC model\n",
    "RVC_MODEL_URL = \"https://huggingface.co/Juneuarie/SilverWolfEN/resolve/main/SilverWolf.zip?download=true\"\n",
    "!python tools/download_all_assets.py --rvc-model \"{RVC_MODEL_URL}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3.4: Test RVC initialization\n",
    "%cd /content/triton-spark-server\n",
    "\n",
    "test_rvc_script = '''\n",
    "import sys\n",
    "sys.path.insert(0, \".\")\n",
    "\n",
    "from rvc import init_rvc, get_config\n",
    "\n",
    "init_rvc()\n",
    "config = get_config()\n",
    "print(f\"RVC initialized:\")\n",
    "print(f\"  Device: {config.device}\")\n",
    "print(f\"  Half precision: {config.is_half}\")\n",
    "print(f\"  GPU: {config.gpu_name}\")\n",
    "'''\n",
    "\n",
    "with open(\"test_rvc_init.py\", \"w\") as f:\n",
    "    f.write(test_rvc_script)\n",
    "\n",
    "!python test_rvc_init.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 4: Start Triton Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4.1: Start Triton server in background\n",
    "# Note: udocker shares host network, so Triton will be accessible at localhost:8001\n",
    "import subprocess\n",
    "import time\n",
    "import os\n",
    "\n",
    "pwd = os.getcwd()\n",
    "\n",
    "server_cmd = f'''udocker --allow-root run \\\n",
    "    --hostauth --hostenv \\\n",
    "    {gpu_config['all_options']} \\\n",
    "    --env=PYTHONPATH=/workspace \\\n",
    "    --volume={pwd}:/workspace \\\n",
    "    triton_server \\\n",
    "    /bin/bash -c \"cd /workspace/triton && tritonserver --model-repository=./model_repo_test\"'''\n",
    "\n",
    "# Run in background\n",
    "process = subprocess.Popen(\n",
    "    f\"nohup {server_cmd} > server_log.txt 2>&1 &\",\n",
    "    shell=True\n",
    ")\n",
    "\n",
    "print(\"Triton server starting...\")\n",
    "print(\"Waiting 45 seconds for models to load...\")\n",
    "time.sleep(45)\n",
    "!tail -30 server_log.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4.2: Test Triton connection from host Python\n",
    "!python tools/test_triton_connection.py --addr localhost --port 8001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 5: Upload Reference Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5.1: Upload reference audio for voice cloning\n",
    "import os\n",
    "import soundfile as sf\n",
    "from google.colab import files\n",
    "\n",
    "# Create directories\n",
    "!mkdir -p references TEMP/tts TEMP/rvc TEMP/test\n",
    "\n",
    "# Upload reference audio\n",
    "print(\"Please upload a reference audio file (.wav format):\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Process uploaded file\n",
    "reference_audio = list(uploaded.keys())[0]\n",
    "references_path = os.path.join(\"references\", reference_audio)\n",
    "\n",
    "with open(references_path, \"wb\") as f:\n",
    "    f.write(uploaded[reference_audio])\n",
    "print(f\"Saved to {references_path}\")\n",
    "\n",
    "# Check sample rate\n",
    "audio, sr = sf.read(references_path)\n",
    "print(f\"Original: {len(audio)/sr:.2f}s @ {sr}Hz\")\n",
    "\n",
    "if sr != 16000:\n",
    "    # Use ffmpeg for resampling (available in Colab)\n",
    "    base_name = os.path.splitext(reference_audio)[0]\n",
    "    reference_audio_16k = os.path.join(\"references\", f\"{base_name}_16k.wav\")\n",
    "    !ffmpeg -y -i \"{references_path}\" -ar 16000 \"{reference_audio_16k}\" -loglevel error\n",
    "    print(f\"Resampled to 16kHz: {reference_audio_16k}\")\n",
    "else:\n",
    "    reference_audio_16k = references_path\n",
    "    print(f\"Already 16kHz, using: {reference_audio_16k}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 6: Run Full Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6.1: Configuration\n",
    "# Modify these settings as needed\n",
    "\n",
    "# Text to synthesize\n",
    "TARGET_TEXT = \"Hello! This is a test of the Triton Spark TTS and RVC voice conversion pipeline. The voice should sound like the reference audio.\"\n",
    "\n",
    "# Reference text (can be empty for offline mode)\n",
    "REFERENCE_TEXT = \"\"\n",
    "\n",
    "# RVC model name (check assets/weights/ for available models)\n",
    "# The SilverWolf model extracts as SilverWolf_e300_s6600.pth\n",
    "RVC_MODEL = \"SilverWolf_e300_s6600.pth\"\n",
    "\n",
    "# Pitch shift in semitones (0 = no change)\n",
    "PITCH_SHIFT = 0\n",
    "\n",
    "# F0 extraction method (rmvpe is best quality)\n",
    "F0_METHOD = \"rmvpe\"\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Text: {TARGET_TEXT[:50]}...\")\n",
    "print(f\"  RVC Model: {RVC_MODEL}\")\n",
    "print(f\"  Pitch Shift: {PITCH_SHIFT}\")\n",
    "print(f\"  F0 Method: {F0_METHOD}\")\n",
    "\n",
    "# List available models\n",
    "print(f\"\\nAvailable models in assets/weights/:\")\n",
    "!ls -la assets/weights/ 2>/dev/null || echo \"  (directory not yet created)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6.2: Run full pipeline test\n",
    "import subprocess\n",
    "\n",
    "cmd = [\n",
    "    \"python\", \"tools/test_pipeline.py\",\n",
    "    \"--triton-addr\", \"localhost\",\n",
    "    \"--triton-port\", \"8001\",\n",
    "    \"--prompt-audio\", reference_audio_16k,\n",
    "    \"--prompt-text\", REFERENCE_TEXT,\n",
    "    \"--text\", TARGET_TEXT,\n",
    "    \"--rvc-model\", RVC_MODEL,\n",
    "    \"--pitch-shift\", str(PITCH_SHIFT),\n",
    "    \"--f0-method\", F0_METHOD,\n",
    "    \"--output-dir\", \"./TEMP/test\"\n",
    "]\n",
    "\n",
    "result = subprocess.run(cmd)\n",
    "print(f\"\\nPipeline finished with code: {result.returncode}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6.3: Play output audio\n",
    "from IPython.display import Audio, display\n",
    "import os\n",
    "\n",
    "tts_path = \"TEMP/test/tts_output.wav\"\n",
    "rvc_path = \"TEMP/test/rvc_output.wav\"\n",
    "\n",
    "if os.path.exists(tts_path):\n",
    "    print(\"TTS Output (before RVC):\")\n",
    "    display(Audio(tts_path))\n",
    "else:\n",
    "    print(f\"TTS output not found at {tts_path}\")\n",
    "\n",
    "if os.path.exists(rvc_path):\n",
    "    print(\"\\nRVC Output (final voice):\")\n",
    "    display(Audio(rvc_path))\n",
    "else:\n",
    "    print(f\"RVC output not found at {rvc_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 7: RVC Server with Parallel Workers\n",
    "\n",
    "This section uses the RVC Server architecture with multiple worker **processes** for true parallel processing.\n",
    "Each worker process has its own copy of the RVC model in GPU memory, bypassing Python's GIL for maximum throughput."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 7.1: Start RVC Server (Persistent Background Process)\n# This starts the RVC server daemon that persists until manually stopped.\n# Similar to how Triton server runs in background (Cell 4.1).\n\nimport subprocess\nimport time\nimport os\n\n# Configuration\nNUM_RVC_WORKERS = 2  # 2 for T4, up to 4 for A100\nRVC_MODEL = \"SilverWolf_e300_s6600.pth\"\n\nprint(f\"Starting RVC server daemon...\")\nprint(f\"  Model: {RVC_MODEL}\")\nprint(f\"  Workers: {NUM_RVC_WORKERS}\")\n\n# Start daemon\n!python tools/rvc_server_control.py start --model \"{RVC_MODEL}\" --workers {NUM_RVC_WORKERS} --timeout 120\n\n# Check status\nprint(\"\\nServer status:\")\n!python tools/rvc_server_control.py status\n\n# Show GPU memory\nprint(\"\\nGPU memory usage:\")\n!nvidia-smi --query-gpu=memory.used,memory.total --format=csv"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7.2: Configure Pipeline Test\n",
    "# Large text chunk to test parallel processing (10 sentences)\n",
    "LARGE_TEXT = \"\"\"\n",
    "Hello! This is the first sentence of our test. It should be processed by the TTS worker first.\n",
    "The second sentence comes next, demonstrating the parallel processing capabilities.\n",
    "Now we have a third sentence to add more work for the RVC workers.\n",
    "Fourth sentence here, which will be queued for voice conversion.\n",
    "The fifth sentence continues our comprehensive test of the pipeline.\n",
    "Sentence number six is being processed through the system.\n",
    "Here comes the seventh sentence for additional testing.\n",
    "The eighth sentence adds more content to our test batch.\n",
    "Ninth sentence in our sequence of test utterances.\n",
    "And finally, the tenth sentence completes our test batch.\n",
    "\"\"\".strip()\n",
    "\n",
    "# Pitch shift in semitones (0 = no change)\n",
    "PITCH_SHIFT = 0\n",
    "\n",
    "# F0 extraction method (rmvpe is best quality)\n",
    "F0_METHOD = \"rmvpe\"\n",
    "\n",
    "print(f\"Pipeline Test Configuration:\")\n",
    "print(f\"  Sentences: {len([s for s in LARGE_TEXT.split('.') if s.strip()])}\")\n",
    "print(f\"  Pitch Shift: {PITCH_SHIFT}\")\n",
    "print(f\"  F0 Method: {F0_METHOD}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7.3: Run Pipeline Test (uses existing RVC server)\n",
    "# The pipeline will detect and use the RVC server started in Cell 7.1\n",
    "# Since the server is already running, this should be fast (~0.5s per fragment for RVC)\n",
    "\n",
    "!python tools/test_rvc_server.py \\\n",
    "    --triton-addr localhost \\\n",
    "    --triton-port 8001 \\\n",
    "    --prompt-audio \"{reference_audio_16k}\" \\\n",
    "    --text \"{LARGE_TEXT}\" \\\n",
    "    --pitch-shift {PITCH_SHIFT} \\\n",
    "    --f0-method {F0_METHOD}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7.4: Shutdown Servers (Free GPU Memory)\n",
    "# RVC server persists between runs for faster subsequent processing.\n",
    "# Run this cell to manually shutdown and free GPU memory.\n",
    "\n",
    "# Shutdown RVC server\n",
    "print(\"Shutting down RVC server...\")\n",
    "!python tools/rvc_server_control.py stop\n",
    "\n",
    "# Shutdown Triton server (Spark TTS)\n",
    "print(\"\\nShutting down Triton server...\")\n",
    "!pkill -f tritonserver\n",
    "\n",
    "# Wait for processes to terminate\n",
    "import time\n",
    "time.sleep(2)\n",
    "\n",
    "# Verify GPU memory is cleared\n",
    "print(\"\\nGPU status after shutdown:\")\n",
    "!nvidia-smi"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}